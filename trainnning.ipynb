{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d34804-70a0-4608-8922-7b4a3cd105bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States for SonicTheHedgehog-Genesis: ['GreenHillZone.Act1', 'GreenHillZone.Act2', 'GreenHillZone.Act3', 'LabyrinthZone.Act1', 'LabyrinthZone.Act2', 'LabyrinthZone.Act3', 'MarbleZone.Act1', 'MarbleZone.Act2', 'MarbleZone.Act3', 'ScrapBrainZone.Act1', 'ScrapBrainZone.Act2', 'SpringYardZone.Act1', 'SpringYardZone.Act2', 'SpringYardZone.Act3', 'StarLightZone.Act1', 'StarLightZone.Act2', 'StarLightZone.Act3']\n",
      "Loading existent model: model-sonic/best_model_1730000\n",
      "Model saved in: model-sonic/best_model_1731000\n",
      "Time steps: 1000, Average Reward: 0.8396921165089857, Best Reward: 2.113521845434251\n",
      "Model saved in: model-sonic/best_model_1732000\n",
      "Time steps: 2000, Average Reward: 0.7772278601041831, Best Reward: 2.113521845434251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-127:\n",
      "Process ForkServerProcess-126:\n",
      "Process ForkServerProcess-123:\n",
      "Process ForkServerProcess-122:\n",
      "Process ForkServerProcess-128:\n",
      "Process ForkServerProcess-121:\n",
      "Process ForkServerProcess-124:\n",
      "Process ForkServerProcess-117:\n",
      "Process ForkServerProcess-125:\n",
      "Process ForkServerProcess-119:\n",
      "Process ForkServerProcess-118:\n",
      "Process ForkServerProcess-120:\n",
      "Process ForkServerProcess-115:\n",
      "Process ForkServerProcess-113:\n",
      "Process ForkServerProcess-116:\n",
      "Process ForkServerProcess-114:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 477\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone finded, starting from zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    462\u001b[0m     model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    463\u001b[0m                 env, \n\u001b[1;32m    464\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 ent_coef\u001b[38;5;241m=\u001b[39mENT_COEF\n\u001b[1;32m    475\u001b[0m             )\n\u001b[0;32m--> 477\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEP_NUMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdonkey_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    480\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:207\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m approx_kl_divs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Do a complete pass on the rollout buffer\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Convert discrete action from float to long\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/buffers.py:505\u001b[0m, in \u001b[0;36mRolloutBuffer.get\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    503\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/buffers.py:519\u001b[0m, in \u001b[0;36mRolloutBuffer._get_samples\u001b[0;34m(self, batch_inds, env)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_samples\u001b[39m(\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    510\u001b[0m     batch_inds: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    511\u001b[0m     env: Optional[VecNormalize] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    512\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RolloutBufferSamples:\n\u001b[1;32m    513\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations[batch_inds],\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[batch_inds],\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantages[batch_inds]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[0;32m--> 519\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_inds\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RolloutBufferSamples(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_torch, data)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import gymnasium as gym\n",
    "import re\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, WarpFrame\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "def custom_schedule(progress_remaining):\n",
    "    #return 1e-4 + (3e-4 - 1e-4) * (1 - progress_remaining)\n",
    "    return 5e-5 + (2e-4 - 5e-5) * (1 - progress_remaining)\n",
    "\n",
    "# Model Param\n",
    "CHECK_FREQ_NUMB = 1000\n",
    "TOTAL_TIMESTEP_NUMB = 500_000_000\n",
    "LEARNING_RATE = custom_schedule # 0.00025 # 0.0002 # 0.0001 # 0.00025 # 0.0001\n",
    "GAE = 0.98 # 0.9 # 1.0 # 0.95 # 1.0\n",
    "ENT_COEF = 0.004 # 0.001 # 0.03 # 0.01 # 0.03 # 0.1 # 0.03 # 0.02 # 0.01 # 0.005 # 0.01\n",
    "N_STEPS = 2048 # 4096 # 512 # 2048 # 4096 # 2048 # 512\n",
    "GAMMA = 0.99 # 0.9\n",
    "BATCH_SIZE = 512 # 128 # 64\n",
    "CLIP_RANGE = 0.1 # 0.15 # 0.2 # 0.4 # 0.3\n",
    "N_EPOCHS = 10 # 6 # 10 # 15 # 10\n",
    "MAX_EPISODE=12000\n",
    "USE_CURRICULUM=False\n",
    "USE_CLIP_REWARD=False\n",
    "STATE=\"GreenHillZone.Act1\"\n",
    "TENSORBOARD=\"./tensorboard-sonic\"\n",
    "# TENSORBOARD=\"./tensorboard-stf2-features/\"\n",
    "SAVE_DIR=\"./model-sonic\"\n",
    "# SAVE_DIR=\"./model-stf2-features\"\n",
    "NUM_ENV = 16\n",
    "\n",
    "model = None\n",
    "\n",
    "# Test Param\n",
    "EPISODE_NUMBERS = 20\n",
    "SAVE_FREQ=1000\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "game = \"SonicTheHedgehog-Genesis\"\n",
    "states = retro.data.list_states(game)\n",
    "\n",
    "print(f\"States for {game}: {states}\")\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "class MainDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            combos=[\n",
    "                [\"LEFT\"],\n",
    "                [\"RIGHT\"],\n",
    "                [\"B\"],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "class IgnorePauseActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        act[3] = 0\n",
    "        return act\n",
    "\n",
    "\n",
    "class ResetStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.x_last = 0\n",
    "        self.lose_lives = False\n",
    "        self.old_rings = 0\n",
    "        #self.current_health = 40\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        #self.x_last = self.env.unwrapped.data['x']\n",
    "\n",
    "        self.steps = 0\n",
    "        self.old_rings = 0\n",
    "        #self.current_health = 40\n",
    "\n",
    "        self.lose_lives = False\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # reward = 0\n",
    "        if reward > 0:\n",
    "            reward = 0.1 # new score reward\n",
    "\n",
    "        # Penalty for excessive jumping\n",
    "        if action == 2:\n",
    "            reward -= 0.3\n",
    "        \n",
    "        # Existential\n",
    "        reward -= 0.05\n",
    "\n",
    "        max_x = info['screen_x_end']\n",
    "        current_x = info['x']\n",
    "        current_rings = info['rings']\n",
    "\n",
    "        # reward for velocity (make loops, etc.)\n",
    "        if self.x_last > 0:\n",
    "            velocity = current_x - self.x_last\n",
    "\n",
    "            if velocity >= 4:\n",
    "                reward += 0.1\n",
    "            elif velocity == 0: # stopped\n",
    "                reward -= 0.05\n",
    "\n",
    "        # reward for advancing the stage as far as possible\n",
    "        if current_x > self.x_last:\n",
    "            reward += (current_x / max_x) * 0.5\n",
    "        elif current_x <= self.x_last:\n",
    "            reward -= 0.02\n",
    "            \n",
    "        self.x_last = current_x\n",
    "\n",
    "        # negative reward for Sonic collide with obstacle\n",
    "        if current_rings < self.old_rings:\n",
    "            reward -= 0.5\n",
    "\n",
    "        self.old_rings = current_rings\n",
    "\n",
    "        if self.steps > MAX_EPISODE:\n",
    "            done = True\n",
    "            reward -= 1\n",
    "\n",
    "        if current_x >= max_x:\n",
    "            reward += 2\n",
    "            done = True\n",
    "\n",
    "        if info['lives'] < 3:\n",
    "            reward -= 1\n",
    "            done = True\n",
    "\n",
    "        \n",
    "\n",
    "        # if self.current_health != info['health'] and info['health'] < self.current_health:\n",
    "        #     reward += (info['health'] - 40) / 40 * 2\n",
    "        #     self.current_health = info['health']\n",
    "\n",
    "        # if info['lives'] < 3 or info['health'] <= 0:\n",
    "        #     reward -= 1\n",
    "        #     done = True \n",
    "        #     self.lose_lives = True\n",
    "\n",
    "        # if done and not self.lose_lives:\n",
    "        #     reward += 2\n",
    " \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "class RandomStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"Select a random state from folder STATE_PATH\"\"\"\n",
    "        STATE_PATH = \"./States\"\n",
    "        states = [f for f in os.listdir(STATE_PATH) if f.endswith(\".state\")]\n",
    "        if not states:\n",
    "            raise FileNotFoundError(\"File not found!\")\n",
    "        c = random.choice(states)\n",
    "\n",
    "        return os.path.abspath(\"./States/\" + c)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state = self.get_random_state()\n",
    "        #print(f\"Loading state: {state}\")\n",
    "        self.env.load_state(state)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        if done or trunc:\n",
    "            self.reset()\n",
    "\n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, required_wins=20): #required_avg_reward=1.0):\n",
    "        super().__init__(env)\n",
    "        self.required_wins = required_wins\n",
    "        #self.required_avg_reward = required_avg_reward\n",
    "        self.current_phase = 1\n",
    "        self.total_wins = 0 \n",
    "        self.rewards_list = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "        could_to_next_stage = info[\"matches_won\"] / self.current_phase >= 2\n",
    "\n",
    "        if info[\"matches_won\"] % 2 == 0 and info[\"matches_won\"] > 0 and could_to_next_stage:\n",
    "            self.total_wins += 1\n",
    "\n",
    "\n",
    "        # avg_reward = np.mean(self.rewards_list[-self.required_wins:]) if len(self.rewards_list) >= self.required_wins else np.mean(self.rewards_list)\n",
    "        avg_reward = np.mean(self.rewards_list)\n",
    "\n",
    "        if could_to_next_stage and \\\n",
    "            ((info[\"matches_won\"] % 2 == 0  and info[\"matches_won\"] > 0) \\\n",
    "                 or (info[\"enemy_matches_won\"] % 2 == 0 and info[\"enemy_matches_won\"] > 0)) :\n",
    "            print(info)\n",
    "            print(f\"🔥 stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            done = True\n",
    "        \n",
    "        if self.total_wins >= self.required_wins: #and avg_reward >= self.required_avg_reward:\n",
    "            self.current_phase += 1\n",
    "            print(f\"🔥 Going to next stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            self.total_wins = 0\n",
    "            self.rewards_list = []\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            observation, reward, terminated, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return observation, total_reward, terminated, trunk, info\n",
    "\n",
    "\n",
    "class GameNet(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(GameNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_freq=SAVE_FREQ,\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.best_reward = float('-inf')\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        reward = self.locals[\"rewards\"][0]\n",
    "        self.current_episode_reward = reward\n",
    "\n",
    "        done = self.locals[\"dones\"][0]\n",
    "\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "\n",
    "        if done:\n",
    "            self.current_episode_reward = 0\n",
    "            self.episode_rewards = []\n",
    "            self.best_reward = float('-inf')\n",
    "        \n",
    "        if self.n_calls % self.check_freq == 0 and len(self.episode_rewards) > 0:\n",
    "            latest_model = get_latest_model(self.save_path)\n",
    "            next_save_step = (int(re.search(r\"best_model_(\\d+)\", str(latest_model)).group(1)) + self.check_freq) if latest_model else self.n_calls\n",
    "            model_path = self.save_path / f\"best_model_{next_save_step}\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved in: {model_path}\")\n",
    "\n",
    "            average_reward = sum(self.episode_rewards) / len(self.episode_rewards)\n",
    "            best_reward = max(self.episode_rewards)\n",
    "            sum_rewards = sum(self.episode_rewards)\n",
    "\n",
    "            self.best_reward = max(self.best_reward, best_reward)\n",
    "\n",
    "            self.logger.record(\"average_reward\", average_reward)\n",
    "            self.logger.record(\"best_reward\", self.best_reward)\n",
    "            self.logger.record(\"sum_rewards\", sum_rewards)\n",
    "\n",
    "            if USE_CURRICULUM:\n",
    "                self.logger.record(\"current_phase\", self.training_env.get_attr(\"current_phase\")[0])\n",
    "\n",
    "            print(f\"Time steps: {self.n_calls}, Average Reward: {average_reward}, Best Reward: {self.best_reward}\")\n",
    "\n",
    "\n",
    "        return True\n",
    "          \n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=GameNet,\n",
    "    features_extractor_kwargs=dict(features_dim=1024), # features_extractor_kwargs=dict(features_dim=512),\n",
    "    # net_arch=dict(\n",
    "    #     pi=[1024, 512, 256],  # Actor\n",
    "    #     vf=[1024, 1024, 512]  # Critic\n",
    "    # ) #\n",
    ")\n",
    "\n",
    "def get_latest_model(path):\n",
    "    models = list(path.glob(\"best_model_*\"))\n",
    "    if not models:\n",
    "        return None\n",
    "    model_numbers = [int(re.search(r\"best_model_(\\d+)\", str(m)).group(1)) for m in models]\n",
    "    latest_model = max(model_numbers)\n",
    "    return path / f\"best_model_{latest_model}\"\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = retro.make(\n",
    "            game=game, \n",
    "            #use_restricted_actions=retro.Actions.DISCRETE, \n",
    "            # render_mode=\"human\",\n",
    "            render_mode=None,\n",
    "            state=STATE\n",
    "        )\n",
    "\n",
    "        # env = DonkeyKongCustomActions(env)\n",
    "\n",
    "        env = MainDiscretizer(env)\n",
    "\n",
    "        # env = IgnorePauseActionWrapper(env)\n",
    "\n",
    "        # env = RandomStateWrapper(env)\n",
    "\n",
    "        env = ResetStateWrapper(env)\n",
    "        \n",
    "        env = SkipFrame(env, skip=4)\n",
    "        env = WarpFrame(env)\n",
    "\n",
    "        if USE_CURRICULUM:\n",
    "            env = CurriculumWrapper(env, required_wins=50) #, required_avg_reward=0.6)\n",
    "\n",
    "        if USE_CLIP_REWARD:\n",
    "            env = ClipRewardEnv(env)\n",
    "\n",
    "        #env = TimeLimit(env, max_episode_steps=MAX_EPISODE)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENV)])\n",
    "# env = DummyVecEnv([make_env()])\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "checkpoint_callback=TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)\n",
    "\n",
    "\n",
    "latest_model_path = get_latest_model(save_dir)\n",
    "\n",
    "if latest_model_path:\n",
    "    print(f\"Loading existent model: {latest_model_path}\")\n",
    "    model = PPO.load(\n",
    "        str(latest_model_path), \n",
    "        env=env, \n",
    "        verbose=0, \n",
    "        tensorboard_log=TENSORBOARD, \n",
    "        learning_rate=LEARNING_RATE, \n",
    "        n_steps=N_STEPS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        n_epochs=N_EPOCHS, \n",
    "        gamma=GAMMA, \n",
    "        gae_lambda=GAE, \n",
    "        clip_range=CLIP_RANGE,\n",
    "        ent_coef=ENT_COEF\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"None finded, starting from zero.\")\n",
    "    model = PPO('CnnPolicy', \n",
    "                env, \n",
    "                verbose=0, \n",
    "                policy_kwargs=policy_kwargs, \n",
    "                tensorboard_log=TENSORBOARD, \n",
    "                learning_rate=LEARNING_RATE, \n",
    "                n_steps=N_STEPS, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                n_epochs=N_EPOCHS, \n",
    "                gamma=GAMMA, \n",
    "                gae_lambda=GAE, \n",
    "                clip_range=CLIP_RANGE,\n",
    "                ent_coef=ENT_COEF\n",
    "            )\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, reset_num_timesteps=False, callback=checkpoint_callback)\n",
    "model.save(\"donkey_final\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0005f91-782c-4ace-9c14-76604df99d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
